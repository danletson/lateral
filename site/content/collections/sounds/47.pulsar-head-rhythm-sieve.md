entry_number: "47"
artist: Marcin Pietruszewski
meta_description: ????
companion_text: |
  <p>The material point of departure for the accompanying track [1] is a new implementation of pulsar synthesis seamlessly integrated with the Distortion Product Otoacoustic Emission (DPOAE) and sieve algorithm [2]. The context of this implementation is my ongoing research project focused on historical techniques of pulsar synthesis – first introduced by Curtis Roads in his book <i>Microsound </i>– its conceptual and programming extensions and using it for composition. The nature and aims of the project are twofold. The first one is technical – an analysis of the original Pulsar Generator program, its source code, underlying programming paradigm and user interface which concludes in the design and development of the New Pulsar Generator program [3]. This technical aim entails a reflection on a wider context of issues related to the process of digital instrument design, musical data representation and human computer interaction. The second aim is artistic creation – a systematic instrumental and compositional practice with the New Pulsar Generator, reflecting on particular formal issues afforded through this process. Intertwined here are retrospective analysis and prospective development, which are mutually dependent [4].
  </p>
  <p>The following text serves as a brief historical and conceptual background to the track ‘<i>(pulsar | head – rhythm) &lt;&gt; sieve’</i>. The bidirectional arrow (&lt; &gt;) in the title indicates the process of osmosis of material (pulsar synthesis) and abstract (analysis descriptors and sieve), phenomenal (auditory) and symbolic (formalised), local (micro-temporal) and global (macro-temporal), and the emergence of timbre and musical form as a pendular movement between them. An investigation of the complex mediation between formalism of abstract procedure acting across and between multiple temporal scales constitutes a pertinent problem in my compositional practice.
  </p>
  <p>I will start with a short introduction of the New Pulsar Generator program, its genealogy within wider sound synthesis categories and its operationalisation of the process of rhythm. The emphasis on the process of rhythm in the conceptualisation of pulsar synthesis allows me to introduce one of many extensions of the program – the analysis of a pulsar train and its re-synthesis with induced DPOAE signal. The rhythm of a pulsar train becomes an instrumentalisation of the very physiological conditions of audition – a <i>head-rhythm</i>. As a conclusion, I would like to introduce the concept of sieve. With its non-temporal nature sieve acts as an organising principle within accompanying work, a pre-existing structural scaffolding modelling the musical form across multiple temporal levels.
  </p>
  <p><strong>The New Pulsar Generator.</strong> The New Pulsar Generator (nuPg) [5] produces a form of synthesis called pulsar synthesis. Pulsar synthesis is a powerful technique of digital sound synthesis named after a highly magnetised rotating neutron star that emits a beam of electromagnetic radiation at a frequency between 0.25 and 642 Hz. The technique operationalises this range as it generates a hybrid of sounds between discrete pulses and continuous tones which cross the perceptual period from infrasonic pulsations to audio frequencies [6].
  </p>
  <p>Pulsar synthesis melds established principles within a new paradigm. Its conceptual origins can be traced back to historical analogue synthesis techniques. In its purest form, PS generates a stream of electronic pulses and pitched tones akin to those produced by analogue instruments designed around the principle of filtered pulse trains (e.g. Ondioline, Hohner Elektronium [7]). However, as a purely digital technique, pulsar synthesis attains the power of precise programmable control and extensibility.
  </p>
  <p>Genealogically, pulsar synthesis belongs to the micro-sound and particle based category of audio synthesis techniques. Aesthetically and conceptually these techniques can be classified as belonging to a wider category of non-standard sound synthesis techniques. The term non-standard has been coined by Stephen R. Holtzman to describe sound synthesis methods that are not based on an acoustical, physical, or psychoacoustic model, but instead utilise abstract concepts of compositional organisation of sound. The non-standard approach, given a set of instructions, relates them to one another in terms of a system which makes no reference to some established model and which relationships themselves are the description of the sound [8]. Within the non-standard paradigm "the computer acts as a sound generating instrument sui generis, not imitating mechanical instruments or theoretical acoustic models" [9]. The emergence of nonstandard sound synthesis systems signified an important conceptual and aesthetic shift brought by the computer and digital domain. The sound production becomes a compositional activity allowing for "the composition of timbre, instead of with timbre"[10]. It allows composition to think beyond a practice concerned with a permutational combinatorics within a closed homogeneous system describable essentially by four properties of pitch, duration, dynamic marking, and instrumental timbre. The emergence of nonstandard sound synthesis systems proposed composition with sound-objects – a heterogeneous endless universe of "liberated sounds" [11]. Horacio Vaggione highlighted this shift in an interview with Osvaldo Budón [12]:
  </p>
  <p><i>The former common paradigm of instrumental music (of which the "total chromatic" serial approach was one of the last manifestations) required a kind of "neutrality of the material", an imperative for a compositional practice that was based on the autonomy of symbolic manipulations. To realize a pure permutational combinatorics, it was necessary to play with notes as "atoms", or primitive building blocks. Here, electroacoustic music caused a real paradigm shift, introducing the sound-object and the idea of morphological multiplicity.</i>
  </p>
  <p>Rather than simply radicalising such dichotomy, the paradigm shift caused by the emergence of electronic music and nonstandard synthesis systems opened a possibility of composition articulating the functionalities particular to each category ("notes", "sound-objects", and beyond) and allowed their integration within a new organisational model. This integrative approach – coalescing the symbolic and iconic representation of musical data [13] – constitutes a pertinent issue in the compositional practice with pulsar synthesis and the development of its digital instrument incarnations – both the historical Pulsar Generator [14] and New Pulsar Generator.
  </p>
  <p><strong>Pulsar Synthesis and Process of Rhythm.</strong> Pulsar Synthesis operationalises the notion of rhythm with its multi-temporal affordances as a system of interconnected patterns evolving on multiple timescales. The technique generates a complex hybrid of sounds across the perceptual time spans between infrasonic pulsations and audio frequencies, giving rise to a broad family of musical structures: singular impulses, sequences, continuous tones, time-varying phrases, and beating textures. Through its inherently multiscale character pulsar synthesis proposes a unique view on rhythm moving beyond a linear sequence of points and intervals tied to a time grid, and proposes a notion of rhythm as a "continuously flowing temporal substrate" [15]. The extended view of rhythm has been proposed by Edgar Varèse [16], who wrote that:
  </p>
  <p><i>Rhythm is too often confused with metrics. Cadence or the regular succession of beats has little to do with rhythm of a composition. Rhythm is the element in music that gives life to a work and holds it together. It is the element of stability, the generator of form. </i>
  </p>
  <p>We can think rhythm generally as any structure or pattern evolving in time. In some cases and specific perceptual limits this pattern can be quantified; we can distinguish the meter and count its basic elements. The scope of rhythm however goes beyond this simple conjunction of onset time and duration. The rhythm "subsumes the undulations internal to a sound, such as accents, swells, vibrato, and tremolo, which can be generalized to fluctuations in any parameter."[17] Ontologically, rather than as a separate entity, the rhythm appears as a construct, a result of the actions –oscillations, modulations and changes of density – in pitch, amplitude, timbre and space [18]. Rhythm’s blurred identity in relation to meter has been highlighted by Eugene Narmour [19] and cognitive studies have shown that listeners are able to construe the rhythmic organisation solely from melodic structure [20]. This composite nature is reflected in rhythm’s unfolding on multiple timescales. As Cooper and Mayer Cooper observed [21]:
  </p>
  <p><i>As a piece of music unfolds, its rhythmic structure is perceived not as a series of discrete independent units strung together, but as an organic process in which smaller rhythmic motives, while possessing shape and structure all their own, also function as integral parts of a larger rhythmic organisation. </i>
  </p>
  <p>The recognition of the continuum between multiple timescales is fundamental to the modern concept of rhythm and lies at the core of practice with pulsar synthesis. Such conceptualisation of rhythm can be viewed as a special case of a rhythm-tone continuum. Henry Cowell described how sped-up discrete rhythms become a continuous tone in 1930 [22]:
  </p>
  <p><i>Rhythm and tone, which been thought to be entirely separate musical fundamentals, are definitely related through overtone ratios. Assume that we have two melodies in parallel to each other, the first written in whole notes and the second in half-notes. If the time for each note were to be indicated by the tapping of a stick, the taps for second melody would recur with double the rapidity of those of the first. If now the taps were to be increased greatly in rapidity without changing the relative speed, it will be seen that when taps for the first melody reach sixteen to the second, those for the second melody will be thirty-two to the second. In other words, the vibrations from the taps of one melody will give the musical tone C, while those of the other will give the tone C one octave higher. Time has translated, as it were, into musical tone. </i>
  </p>
  <p>Karlheinz Stockhausen formulated complementary theory of the continuum between rhythm and pitch in the context of serial music [23]:
  </p>
  <p><i>If the rate of beats is gradually increased beyond the time constant of the filter and the limits beyond which the ear can no longer differentiate, what started as a rhythmically repeated note becomes continuous. We see a continuous transition between what might be called durational intervals characterised as pitch levels. </i>
  </p>
  <p>In the essay “... how time passes...” he extrapolated a unified view of the relationship between the various time scales of musical structure. Stockhausen begins by noting the generality of the concept of period, an interval between two cycles. Period appears in both rhythm (from 6 sec to 1/16th of a sec) and pitch (from about 1/16th sec to about 1/3200th sec). The key here is that pitch and rhythm can be considered as one and the same phenomenon, differing only in their respective time scales. Taking this argument deeper into the micro-temporal domain, the tone colour or steady-state spectrum of a note can also be seen as a manifestation of micro-rhythm over a fundamental frequency. This point of view can also be applied in the macro-temporal domain. Thus, an entire composition can be viewed as one time spectrum of a fundamental duration. Stockhausen proposed the idea of an absolute uniformity of temporality, irrespective of apparent differences between rhythmic and timbral qualities. A lesson from Stockhausen’s essay “... how time passes...” was to show how difficult it is to apply a proportional series developed for a parameter operating on one timescale (e.g., pitch periods) to another operating on different one (e.g., note durations) [25]. In “The Concept of Unity in Electronic Music” Stockhausen expanded this view by looking at the acoustical frequency continuum as a collection of units broken into different phenomena by human perception [25]:
  </p>
  <p><i>In working with impulse generator], one must proceed from a basic concept of a single unified musical time; and the different perceptual categories such as colour, harmony and melody, meter and rhythm, dynamics, and form must be regarded as corresponding to the different components of this unified time. </i>
  </p>
  <p>In the composition <i>Kontakte</i> (1960) realized with assistance from Gottfried Michael Koenig [26] Stockhausen had used an impulse generator to create pulse trains to which he had applied a narrow band-pass filter, which gave each pulse a variable resonance. At narrow band the pulses resonated at a specific pitch. If the pulse train was irregular, the infrasonic impulses generated non-metrical rhythms. By transposing these rhythms – via tape speed manipulations – up into the audible frequency range, Stockhausen was able to build noises from aperiodic impulse trains. The technique of recirculating tape feedback loops was developed in 1951 by Werner Meyer-Eppler, Stockhausen’s teacher [27]. The section between 16:56 and 18:26 of <i>Kontakte</i> features the now iconic transition from rhythm to tone and back to rhythm.
  </p>
  <p>Pulsar synthesis encapsulates this process and renders audible the transfer between rhythmic discrete units and the spectrum of a continuous tone. Perceptually, what on discrete polarity appears as a distinct rhythmic pattern becomes a spectral template on a continuous pole. This continuity – paradigmatic for pulsar synthesis – allows for a flexible modelling of the Distortion Product Otoacoustic Emission that relays perceptual tonal character on rhythmic properties of its beat frequency.
  </p>
  <p><strong></strong><br>
  </p>
  <p><strong>Distortion Product Otoacoustic Emission (DPOAE): Head-Rhythm.</strong> In 1856, Hermann von Helmholtz was the first to identify sum and difference tones as products of the auditory distortion [28]. Helmholtz used two prolonged tones with a harmonic interval of less than an octave played at high volume. When two tones – a lower and a higher – sounded together, a third (combination) tone appeared. The observed tone had the frequency equal to a difference between the two primary tones – e.g. the combination of 100 and 250 Hz played at a strong intensity created a tone of 150 Hz. The combination tone is not being emitted externally, but it is clearly, objectively perceived by the listener [29]. An initial explanation for these phenomena was that high-intensity levels forced the linear mechanics of the physical auditory system into a nonlinear region. The nonlinearity thus was thought to be located in the middle ear or in the basilar membrane [30]. A shift in paradigm were findings of Thomas Gold and David T. Kemp which proved that the ear, rather than being passive, should be considered as an active system, and that parts of the inner ear – specifically, the outer hair cells of the basilar membrane – act as a dynamic amplification system [31][32]. Nowadays, the phenomenon of auditory distortion forms an area of research on otoacoustic emissions, where the “distortion” is defined as a positive feedback mechanism within the cochlea called the cochlear amplifier – a sort of "a tiny loudspeaker in the ear" [33]. In the field of medical practice, the otoacoustic emission phenomena have been used, among other purposes, to test hearing in infants [34]. Distortion Product Otoacoustic Emissions challenge the sound as a solely externally constructed entity and place the listener as both a recipient and an agent of the auditory process. Listening ceases to be indexically oriented towards an external sound source and re-orientates itself towards the very physiological conditions of audition.
  </p>
  <p>Maryanne Amacher contributed significantly to research on auditory distortion and its creative application into sound art. Her compositions such as “Head Rhythm 1” and "Playing Thing 2" utilised the Triadex Muse – a digital sequencer instrument designed and built by Edward Fredkin and Marvin Minsky at the Massachusetts Institute of Technology (MIT) [35]. By generating fast-paced interlocking patterns of short sine tones at very high volumes, Amacher achieved a presence of distinct separate musical streams from the audible distortion. In the liner notes to “Sound Characters (Making of the Third Ear)” a CD released by Tzadik, Amacher describes the experience of these tones:
  </p>
  <p><i>When played at the right sound level, which is quite high and exciting, the tones in this music will cause your ears to act as neurophonic instruments that emit sounds that will seem to be issuing directly from your head ... [my audiences] discover they are producing a tonal dimension of the music which interacts melodically, rhythmically, and spatially with the tones in the room. Tones “dance” in the immediate space of their body, around them like a sonic wrap, cascade inside ears, and out to space in front of their eyes ... Do not be alarmed! Your ears are not behaving strange or being damaged! ... These virtual tones are a natural and very real physical aspect of auditory perception, similar to the fusing of two images resulting in a third three dimensional image in binocular perception ... I want to release this music which is produced by the listener</i>. [36]
  </p>
  <p>Amacher had laid the foundation for systematic exploration of DPOAEs and engagement with their novel musical properties. Jonathan Kirk and Christopher Haworth provided an extensive list of examples from 20th-century music where the auditory distortion product had been explored as an intentional material strategy [37]. Recent examples of creative investigation of phenomena include Christopher Haworth's “Correlation Number One” (2010), Marcus Schmickler's “Fortuna Ribbon” (2015)[38], Thomas Ankersmit “Otolith” (2015) [39], and Florian Hecker's “FAVN” (2017).
  </p>
  <p>The application of DPOAE within accompanying track has been based on the Quadratic ( f2 – f1) and Cubic (2 f1 – f2) Difference algorithms proposed by Kendall, Haworth and Cádiz [40]. The authors have provided a set of synthesis techniques by which the DPOAE's can be used in a determined and creative way with control over dynamic parameters of tone such as vibrato, tremolo, spectra and spatial location. A Dynamic Sinusoidal Synthesis (DDS) model has been used to generate material for the work. In DSS the amplitude and pitch of the QDT spectrum is a result of the analysis process performed on a complex and dynamically changing in time model signal [41]. The working of sound material and form completes at the site of a unique psychology and physiology of the listener; here the ear is conceived as an actual instrument [42].
  </p>
  <p><strong>Listening to the Sieve. </strong>Hugo Reimann coined a term <i>Beziehendes Denken </i>- which tentatively can be understood as relational hearing - and defined as setting parts of the whole in relation to each other [43]. A relational hearing defined as such is close to the notion of active listening - a strategy for reconstructing the logical flow of composition by conjoining musical patterns. Fundamental in this conceptualisation is the distinction between horizontal and vertical structures. When structural hearing relates to parts in linear order the process focuses on horizontal relations between adjacent or non-adjacent sections. The vertical structures, on the other hand, refer to parts in hierarchical order, whose relations are super- and subordinate to each other. The aim of structural hearing is to extract relationships horizontally and vertically at each hierarchical level - from the most reductive, the <i>Ursatz</i> or deep structure, to the surface structure, the most elaborate level <i>Vordergrund</i>. These two axes delineate an idealised space for the emergence of musical form. It is on these two axes where the formal procedure of sieve has been used extensively within accompanying work. At the micro-temporal level operations on sieves allowed construction of pulsar generator parameters, such as the pulsaret waveform and the pulsaret envelope. At the meso-time level sieve has the sieve procedure had been applied to generate a pattern of pulsar masking effect. At the macro-timesacle level I have used sieves to generate durations of consecutive sections, shaping the larger scale development of the work. The sieve (cribles) is a formal tool developed by composer Iannis Xenakis for construction of integer-sequence generators that can be applied to generation of various numerical patterns to represent any set of parameters of sound - or well- ordered sound structures - such as pitch series, time-points, loudness, density, degrees of order or local timbres [41]. In mathematics, the term ‘sieve’ is a metaphor for a process of abstraction, a set-theoretical filtering-out that generates series of integers, particularly by means of a modulo operation, that exhibit intervallic repetition [42]. Xenakis used sieves to generate multiple subdivisions of a given span applying different moduli. He then employed logical operators such as intersection or union to construct series that select different elements from the various subdivisions. The power of sieve formula proposed by Xenakis is that it is independent of time. The material generated through this procedure belongs to outside-of-time domain - which means that the way this material will unfold in time is not yet specified. The outside- of-time refers to any aspect of a work of music that can be formalised independently of time. Any other aspect, particularly if dependent on the time flow, belongs to the in- time domain. A 12-ton row, considered in its precompositional, theoretical state, is outside-of-time, while a particular realisation of this series in a score happens in-time. Xenakis used sieves in many of his instrumental compositions, the best documented being <i>Herma</i> for piano solo [44].The process of composition with pulsars and their DPOAE re-synthesis in conjunction with <i>sieves</i> mobilises a dynamic extension to a hierarchical stratification of auditory processing proposed by Reimann. The relational hearing becomes mediated through various abstractions of musical processes - as a direct experimentation with the micro-temporal emergent sonorities of the synthetic material, as its analysis and re-synthesis and as an outside-time formula.
  </p>
  <p><strong>Acknowledgment.</strong> I would like to thank Eric Laska for giving me the opportunity to presentthis sound work with accompanying text. Florian Hecker and Martin Parker for supervision of my research project and ongoing support of my creative work. Curtis Roads and Alberto de Campo for sharing the historical pulsar synthesis and Pulsar Generator code and commenting on the development of the New Pulsar Generator. Marcus Schmickler for advice on aesthetic and technical extensions of the program.
  </p>
  <p><br>
  </p>
  <hr>
  <p><a href="#_ednref" name="_edn1" title=""></a>[1] The track accompanying this short essay is a variation on and an edit out from a forthcoming full-length CD 'The New Pulsar Generator Recordings' to be released on FANCYYYYY label in early 2019. <a href="http://www.fancyyyyy.com/">http://www.fancyyyyy.com/</a>
  </p>
  <p><a href="#_ednref" name="_edn2" title=""></a>[2] SuperCollider implementation of the sieve algorithm developed by Daniel Meyer (<a href="https://www.daniel-mayer.at/software_en.htm">https://www.daniel-mayer.at/software_en.htm</a>) and Python by Christopher Ariza (<a href="https://pypi.org/project/athenaCL/)">https://pypi.org/project/athenaCL/)</a>
  </p>
  <p><a href="#_ednref" name="_edn3" title=""></a>
  </p>
  <p>[3] The code analysed included early (1997) implementation of pulsar synthesis in SuperCollider 1 programming language developed by Curtis Roads and Steven T. Pope as well as a later one (2001) which eventually became Pulsar Generator program developed together with Alberto de Campo
  </p>
  <p><a href="#_ednref" name="_edn4" title=""></a>
  </p>
  <p>[4] See m<ins>y institutional bio</ins>: <a href="https://www.eca.ed.ac.uk/profile/marcin-pietruszewski">https://www.eca.ed.ac.uk/profile/marcin-pietruszewski</a>
  </p>
  <p><a href="#_ednref" name="_edn5" title=""></a>
  </p>
  <p>[5] More information about the program and its documentation can be viewed here: <a href="https://www.marcinpietruszewski.com/the-new-pulsar-generator">https://www.marcinpietruszewski.com/the-new-pulsar-generator</a>
  </p>
  <p><a href="#_ednref" name="_edn6" title=""></a>
  </p>
  <p>[6] For detailed explanation of the technique see Roads, C. [2015]. Composing electronic music: a new aesthetic, Oxford University Press, USA and Roads, C. [2004]. Microsound, MIT press.
  </p>
  <p><a href="#_ednref" name="_edn7" title=""></a>[7] Fourier, L., Roads, C. and Perrey, J.-J. [1994]. Jean-jacques perrey and the ondioline, Computer Music Journal 18(4): 19–25.
  </p>
  <p><a href="#_ednref" name="_edn8" title=""></a>
  </p>
  <p>[8] Holtzman, S. R. [1978]. A description of an automated digital sound synthesis instrument, University of Edinburgh, Department of Artificial Intelligence.
  </p>
  <p><a href="#_ednref" name="_edn9" title=""></a>[9] Koenig, G. M. [1978]. Composition processes, UNESCO Computer Music: Report on an International Project Including the International Workshop Held at Aarhus, Denmark in, pp. 105–126.
  </p>
  <p><a href="#_ednref" name="_edn10" title=""></a>[10] Brün, H. [2004]. When music resists meaning: the major writings of Herbert Brün, Wesleyan University Press.
  </p>
  <p><a href="#_ednref" name="_edn11" title=""></a>[11] Varèse, E. and Wen-Chung, C. [1966]. The liberation of sound, Perspectives of new music 5(1): 11–19.
  </p>
  <p><a href="#_ednref" name="_edn12" title=""></a>[12] Budón, O. [2000]. Composing with objects, networks, and time scales: an interview with horacio vaggione, Computer Music Journal 24(3): 9–22.
  </p>
  <p><a href="#_ednref" name="_edn13" title=""></a>[13] On a problem of representation of musical data see: Müller, M. [2015]. Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications, Springer and Roads, C. and Wieneke, P. [1979]. Grammars as representations for music, Computer Music Journal pp. 48–55.
  </p>
  <p><a href="#_ednref" name="_edn14" title=""></a>[14] The original OS 9 Pulsar Generator can be downloaded from Curtis Roads’ webpage: <a href="https://www.curtisroads.net/software/">https://www.curtisroads.net/software/</a>
  </p>
  <p><a href="#_ednref" name="_edn15" title=""></a>
  </p>
  <p>[15] Roads, C. [2015]. Composing electronic music: a new aesthetic, Oxford University Press, USA.
  </p>
  <p><a href="#_ednref" name="_edn16" title=""></a>[16] Varèse, E. [1967]. Rhythm, form and content, Contemporary Composers on Contemporary Music. New York: Norton.
  </p>
  <p><a href="#_ednref" name="_edn17" title=""></a>[17] Composing electronic music: a new aesthetic, p. 137
  </p>
  <p><a href="#_ednref" name="_edn18" title=""></a>
  </p>
  <p>[18] Berry, W. [1976]. Structural functions in music, Courier Corporation.
  </p>
  <p><a href="#_ednref" name="_edn19" title=""></a>[18] Narmour, E. [1984]. Toward an analytical symbology: The melodic, harmonic and durational functions of implication and realization, Musical Grammars and Computer Analysis: Atti del Convegno. Florence: Olschki pp. 83–114.
  </p>
  <p><a href="#_ednref" name="_edn20" title=""></a>[19] Ahlbäck, S. [2004]. Melody beyond notes. a study of melody cognition.
  </p>
  <p><a href="#_ednref" name="_edn21" title=""></a>[20] Cooper, G. W., Cooper, G. and Meyer, L. B. [1963]. The rhythmic structure of music, University of Chicago press.
  </p>
  <p><a href="#_ednref" name="_edn22" title=""></a>[21] Cowell, H. and Nicholls, D. [1996]. New musical resources, Cambridge University Press.
  </p>
  <p><a href="#_ednref" name="_edn23" title=""></a>[22] Stockhausen, K. [1958]. Structure and experiential time, Die Reihe 2: 64–74.
  </p>
  <p><a href="#_ednref" name="_edn24" title=""></a>[23] Stockhausen, K. [1959]. How time passes, die Reihe 3: 10–40.
  </p>
  <p><a href="#_ednref" name="_edn25" title=""></a>[24] Stockhausen, K. and Barkin, E. [1962]. The concept of unity in electronic music, Perspectives of New Music pp. 39–48.
  </p>
  <p><a href="#_ednref" name="_edn26" title=""></a>[25] Supper, M. [1997]. Elektroakustische Musik und Computermusik: Geschichte, Ästhetik, Methoden, Systeme, Wolke.
  </p>
  <p><a href="#_ednref" name="_edn27" title=""></a>[26] Ungeheuer, E. S. [1992]. Wie die elektronische Musik erfunden wurde...: Textbd, Schott.
  </p>
  <p><a href="#_ednref" name="_edn28" title=""></a>[27] In 1748 and 1754 respectively, the organist, Georg Sorge (1703–1778), and the violinist, Giuseppe Tartini (1692–1770) both observed the phenomena of "a third tone". For a history of combination tones before Helmholtz, see:Carlton Maley Jr, V. [1990]. The theory of beats and combination tones, 1700- 1863.[harvard dissertations in the history of science. O. Gingerich, editor].
  </p>
  <p><a href="#_ednref" name="_edn29" title=""></a>[28] Jennie Gottschalk asks: "If the sound happens exclusively within the hearing mechanism, is it truly objective?" and suggests the answer - "it could be called physically subjective, yet mentally objective", see: Gottschalk, J. [2016]. Experimental Music Since 1970, Bloomsbury Publishing USA.
  </p>
  <p><a href="#_ednref" name="_edn30" title=""></a>
  </p>
  <p>[29] Helmholtz, H. [2013]. On the sensations of tone, Courier Corporation.
  </p>
  <p><a href="#_ednref" name="_edn31" title=""></a>
  </p>
  <p>[30] Gold, T. [1948]. Hearing. ii. the physical basis of the action of the cochlea, Proceedings of the Royal Society of London B: Biological Sciences 135(881): 492–498.
  </p>
  <p><a href="#_ednref" name="_edn32" title=""></a>
  </p>
  <p>[31] Kemp, D. T. [1978]. Stimulated acoustic emissions from within the human auditory system, The Journal of the Acoustical Society of America 64(5): 1386–1391.
  </p>
  <p><a href="#_ednref" name="_edn33" title=""></a>
  </p>
  <p>[32] Ashmore, J., Avan, P., Brownell, W., Dallos, P., Dierkes, K., Fettiplace, R., Grosh, K., Hackney, C., Hudspeth, A., Jülicher, F. et al. [2010]. The remarkable cochlear amplifier, Hearing research 266(1): 1–17.
  </p>
  <p><a href="#_ednref" name="_edn34" title=""></a>[33] See: <a href="https://www.ncbi.nlm.nih.gov/pubmed/12764789">https://www.ncbi.nlm.nih.gov/pubmed/12764789</a>.
  </p>
  <p><a href="#_ednref" name="_edn35" title=""></a>[34] see <a href="https://www.youtube.com/watch?v=YZVyl8HkxWU">https://www.youtube.com/watch?v=YZVyl8HkxWU</a>
  </p>
  <p><a href="#_ednref" name="_edn36" title=""></a>
  </p>
  <p>[35] Amacher, M. [1999]. Sound characters:(making the third ear), Tzadik. Hear ‘Head Rhythm 1’ <a href="https://www.youtube.com/watch?v=_MahrtRVhkA">here</a>.
  </p>
  <p><a href="#_ednref" name="_edn37" title=""></a>[36]  see Kirk, J. [2010]. Otoacoustic emissions as a compositional tool, Ann Arbor, MI: Michigan Publishing, University of Michigan Library.
  </p>
  <p>and Haworth, C. [2011]. Composing with absent sound, ICMC.
  </p>
  <p><a href="#_ednref" name="_edn38" title=""></a>[37] <a href="https://vimeo.com/channels/1137214/126234679">https://vimeo.com/channels/1137214/126234679</a>
  </p>
  <p><a href="#_ednref" name="_edn39" title=""></a>
  </p>
  <p>[38]  listen to an excerpt here: <a href="https://soundcloud.com/weerzin/live-at-ctm-2015-excerpt">https://soundcloud.com/weerzin/live-at-ctm-2015-ex...</a>
  </p>
  <p><a href="#_ednref" name="_edn40" title=""></a>
  </p>
  <p>[39] Kendall, G. S., Haworth, C. and Cádiz, R. F. [2014]. Sound synthesis with auditory distortion products, Computer Music Journal .
  </p>
  <p><a href="#_ednref" name="_edn41" title=""></a>[40] ibid, 13
  </p>
  <p><a href="#_ednref" name="_edn42" title=""></a>
  </p>
  <p>[41] Haworth, C. [2012]. Ear as instrument, Leonardo Music Journal 22: 61–62.
  </p>
  <p>[42] Neuhaus, C., Knösche, T. R. and Friederici, A. D. [2009]. Similarity and repetition, Annals of the New York Academy of Sciences 1169(1): 485–489
  </p>
  <p>[43] Neuhaus, C., Knösche, T. R. and Friederici, A. D. [2009]. Similarity and repetition, Annals of the New York Academy of Sciences 1169(1): 485–489
  </p>
  <p>[44] For an in-depth computer aided analysis of the work and underlying sieve procedure see: Agon, C., Andreatta, M., Assayag, G., & Schaub, S. (2004) Formal Aspects of Iannis Xenakis' "Symbolic Music": A Computer-Aided Exploration of Compositional Processes, Journal of New Music Research, 33:2, 145-159
  </p>
pub_date: 2018-11-25
sound_url: http://feeds.soundcloud.com/stream/540927240-lateral-addition-sieve.mp3
release_group:
  - Late 2018
title: '(pulsar | head – rhythm) <> sieve'
id: 1ea07c84-3337-42be-ab9b-7f73d5945d72
